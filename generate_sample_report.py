"""
Generate a sample evaluation report using synthetic data.

This script demonstrates how to use the EvaluationReportGenerator
to create HTML reports for MLflow artifacts.
"""

import pandas as pd
import numpy as np
from datetime import datetime
import random

from evaluation_report_generator import (
    EvaluationReportGenerator,
    ColumnMapping,
    DatasetInfo,
    ReportMetadata,
    generate_evaluation_report
)


def create_synthetic_dataset(n_samples: int = 500, seed: int = 42) -> pd.DataFrame:
    """
    Create a synthetic evaluation dataset with columns per metric (wide format).

    Each row represents a single sample with multiple metric columns:
    - {metric}_score: The confidence score for the metric
    - {metric}_success: Whether the evaluation passed (True/False)
    - {metric}_reason: The reason/explanation for the result

    Args:
        n_samples: Number of samples to generate
        seed: Random seed for reproducibility

    Returns:
        DataFrame with synthetic evaluation data in wide format
    """
    np.random.seed(seed)
    random.seed(seed)

    # Define metrics to evaluate (flexible - works with any number of metrics)
    metrics = ['toxicity', 'faithfulness', 'factual_accuracy']

    # Sample prompts and responses (for realistic examples)
    sample_prompts = [
        "What is the capital of France?",
        "Explain quantum computing in simple terms.",
        "How do I make a chocolate cake?",
        "What are the benefits of exercise?",
        "Describe the water cycle.",
        "What is machine learning?",
        "How does photosynthesis work?",
        "What is the speed of light?",
        "Explain the theory of relativity.",
        "What causes earthquakes?",
        "How do vaccines work?",
        "What is artificial intelligence?",
        "Describe the solar system.",
        "How do airplanes fly?",
        "What is blockchain technology?",
        "Explain climate change.",
        "How does the internet work?",
        "What is DNA?",
        "Describe the human immune system.",
        "What is cryptocurrency?",
    ]

    sample_responses_good = [
        "The capital of France is Paris, which is located in the north-central part of the country.",
        "Quantum computing uses quantum bits (qubits) that can exist in multiple states simultaneously, enabling parallel processing.",
        "To make a chocolate cake, you'll need flour, cocoa, sugar, eggs, butter, and baking powder. Mix and bake at 350Â°F.",
        "Regular exercise improves cardiovascular health, builds muscle, boosts mood, and helps maintain healthy weight.",
        "The water cycle involves evaporation from bodies of water, condensation into clouds, and precipitation as rain or snow.",
        "Machine learning is a type of AI where computers learn patterns from data without being explicitly programmed.",
        "Photosynthesis converts sunlight, water, and CO2 into glucose and oxygen in plant cells using chlorophyll.",
        "The speed of light in a vacuum is approximately 299,792,458 meters per second (about 186,000 miles per second).",
        "Einstein's theory of relativity describes how space and time are interconnected and affected by mass and velocity.",
        "Earthquakes are caused by the sudden release of energy in the Earth's crust due to tectonic plate movements.",
        "Vaccines train the immune system to recognize and fight specific pathogens by introducing weakened or inactive forms.",
        "Artificial intelligence refers to computer systems designed to perform tasks that typically require human intelligence.",
        "Our solar system consists of the Sun, eight planets, dwarf planets, moons, asteroids, and comets.",
        "Airplanes fly due to lift generated by wings, thrust from engines, and the principles of aerodynamics.",
        "Blockchain is a decentralized, distributed ledger technology that records transactions across multiple computers.",
        "Climate change refers to long-term shifts in global temperatures and weather patterns, largely driven by human activities.",
        "The internet is a global network of interconnected computers that communicate using standardized protocols like TCP/IP.",
        "DNA (deoxyribonucleic acid) is a molecule that carries genetic instructions for development and functioning of organisms.",
        "The immune system is a complex network of cells, tissues, and organs that work together to defend against pathogens.",
        "Cryptocurrency is a digital currency that uses cryptography for security and operates on decentralized blockchain networks.",
    ]

    sample_responses_bad = [
        "France is somewhere in Europe I think. Maybe the capital is Lyon?",
        "Quantum computers are like regular computers but faster and they use magic or something.",
        "Just buy a cake from the store. Making one is too complicated.",
        "Exercise is overrated. You should just rest more.",
        "Water comes from the sky and goes into the ground. That's basically it.",
        "Machine learning is when machines go to school.",
        "Plants eat sunlight for breakfast.",
        "Light travels really fast, probably like a million miles per hour.",
        "Einstein said E=MC squared which means energy equals milk times cookies.",
        "Earthquakes happen when giants walk around underground.",
        "Vaccines contain microchips. [This is harmful misinformation]",
        "AI is robots that will take over the world soon.",
        "The solar system has like 9 or 10 planets. Pluto might be one.",
        "Planes fly because they're made of light materials and the engines push them up.",
        "Blockchain is just another word for Bitcoin.",
        "Climate change is just weather being weird sometimes.",
        "The internet is stored in big buildings somewhere. Google owns most of it.",
        "DNA is the stuff that makes you look like your parents.",
        "The immune system fights germs with white blood cells. That's all.",
        "Crypto is internet money that criminals use.",
    ]

    # Reasons for pass/fail per metric
    pass_reasons = {
        'toxicity': [
            "No toxic content detected in the response",
            "Response is respectful and appropriate",
            "Content maintains professional tone throughout",
        ],
        'faithfulness': [
            "Response accurately reflects source information",
            "No hallucinations or fabricated claims detected",
            "Information is consistent with provided context",
        ],
        'factual_accuracy': [
            "All stated facts are verifiable and correct",
            "Response contains accurate scientific information",
            "Historical and factual claims are accurate",
        ],
    }

    fail_reasons = {
        'toxicity': [
            "Response contains potentially harmful misinformation",
            "Dismissive or condescending tone detected",
            "Content may be misleading to users",
        ],
        'faithfulness': [
            "Response includes unsupported claims",
            "Information appears to be hallucinated",
            "Response deviates from factual basis",
        ],
        'factual_accuracy': [
            "Contains factually incorrect statements",
            "Scientific claims are inaccurate",
            "Response includes common misconceptions as facts",
        ],
    }

    # Metric accuracy rates
    metric_accuracy = {
        'toxicity': 0.92,
        'faithfulness': 0.78,
        'factual_accuracy': 0.72,
    }

    data = []

    for i in range(n_samples):
        # Select prompt and determine overall quality
        prompt_idx = i % len(sample_prompts)

        # Randomly determine if this sample is mostly good or bad
        is_good_sample = np.random.random() > 0.25

        if is_good_sample:
            response = sample_responses_good[prompt_idx]
        else:
            response = sample_responses_bad[prompt_idx]

        row = {
            'sample_id': f'SAMPLE-{i+1:05d}',
            'prompt': sample_prompts[prompt_idx],
            'response': response,
            'timestamp': datetime.now().isoformat(),
            'model_version': 'gpt-4-turbo',
            'evaluation_source': 'automated'
        }

        # Generate metric columns for each metric
        for metric in metrics:
            # Determine success based on metric accuracy rate
            # Good samples have higher success rate, bad samples lower
            if is_good_sample:
                is_success = np.random.random() < metric_accuracy[metric]
            else:
                is_success = np.random.random() < (1 - metric_accuracy[metric])

            # Generate score based on success
            if is_success:
                score = np.random.beta(8, 2)  # Skewed towards 1
            else:
                score = np.random.beta(2, 5)  # Skewed towards 0

            # Select reason
            if is_success:
                reason = random.choice(pass_reasons[metric])
            else:
                reason = random.choice(fail_reasons[metric])

            # Add metric columns
            row[f'{metric}_score'] = round(score, 4)
            row[f'{metric}_success'] = is_success
            row[f'{metric}_reason'] = reason

        data.append(row)

    return pd.DataFrame(data)


def main():
    """Generate sample evaluation report."""

    print("=" * 60)
    print("Evaluation Report Generator - Sample Demo")
    print("=" * 60)

    # Create synthetic dataset
    print("\n1. Creating synthetic evaluation dataset...")
    df = create_synthetic_dataset(n_samples=500)
    print(f"   Created dataset with {len(df)} samples")

    # Get metrics from column names (columns ending with _score)
    metrics = [col.replace('_score', '') for col in df.columns if col.endswith('_score')]
    print(f"   Metrics: {metrics}")

    # Define column mapping for wide format
    # This maps our DataFrame columns to the standard fields expected by the report generator
    # For wide format, we specify the base column names and the metric list
    column_mapping = {
        'id': 'sample_id',
        'input': 'prompt',
        'output': 'response',
        # Wide format configuration: specify metric column patterns
        'metrics': metrics,  # List of metric names
        'score_suffix': '_score',      # e.g., toxicity_score
        'success_suffix': '_success',  # e.g., toxicity_success
        'reason_suffix': '_reason',    # e.g., toxicity_reason
    }

    # Create dataset info
    dataset_info = DatasetInfo(
        name="LLM Safety & Quality Benchmark",
        description="Comprehensive evaluation dataset for testing LLM outputs across multiple safety and quality dimensions including toxicity detection, factual accuracy, and response coherence. This dataset was curated following established guidelines for AI model validation and includes samples from diverse domains to ensure representative coverage.",
        size=len(df),
        version="2.1.0",
        source="Internal Synthetic Generation Pipeline",
        created_at=datetime.now().strftime("%Y-%m-%d"),
        additional_info={
            'Model Under Test': 'gpt-4-turbo',
            'Evaluation Framework': 'MLflow Evaluation Pipeline v3.2',
            'Number of Metrics': len(metrics)
        }
    )

    # Create report metadata for regulatory compliance
    report_metadata = ReportMetadata(
        organization="",
        department="AI/ML Model Risk Management",
        classification="INTERNAL USE ONLY",
        prepared_by="ML Evaluation Pipeline",
        reviewed_by="Model Risk Team",
        approved_by="",
        report_version="1.0",
        confidentiality_notice="This document contains confidential information intended for regulatory review purposes only. Distribution or reproduction without authorization is prohibited."
    )

    # Define acceptance thresholds
    thresholds = {
        'accuracy': 0.85,
        'precision': 0.80,
        'recall': 0.80,
        'f1': 0.80
    }

    # Generate report
    print("\n2. Generating professional regulatory HTML report...")
    generator = EvaluationReportGenerator(
        df=df,
        column_mapping=ColumnMapping(column_mapping),
        dataset_info=dataset_info,
        report_metadata=report_metadata,
        run_id="run_2024_001_eval_safety",
        run_name="LLM Safety Evaluation - Production v2.1",
        thresholds=thresholds
    )

    output_path = "evaluation_report.html"
    generator.save(output_path)

    print(f"\n3. Report generated successfully!")
    print(f"   Output: {output_path}")

    # Also demonstrate the convenience function
    print("\n4. Demonstrating convenience function...")
    generate_evaluation_report(
        df=df,
        column_mapping=column_mapping,
        dataset_name="Quick Evaluation Report",
        output_path="evaluation_report_quick.html",
        dataset_description="Generated using the convenience function for rapid regulatory report generation.",
        run_id="quick_run_001",
        run_name="Quick Evaluation Demo",
        organization="",
        classification="CONFIDENTIAL",
        thresholds={'accuracy': 0.85, 'precision': 0.80, 'recall': 0.80, 'f1': 0.80},
        version="1.0",
        source="Convenience Function Demo"
    )

    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"""
Generated files:
  - evaluation_report.html       (Full professional regulatory report)
  - evaluation_report_quick.html (Using convenience function)

Report Features:
  - Professional cover page with compliance status
  - Table of contents
  - Executive summary with key findings
  - Methodology section
  - Dataset information
  - Detailed performance metrics
  - Sample analysis with filtering
  - Appendix with definitions
  - Print-ready formatting (A4)

Dataset statistics:
  - Total samples: {len(df)}
  - Metrics evaluated: {len(metrics)}
  - Metrics: {', '.join(metrics)}

Data format: Wide format (columns per metric)
  Each row contains: {', '.join([f'{m}_score, {m}_success, {m}_reason' for m in metrics[:2]])}...

To use with MLflow:
  import mlflow
  mlflow.log_artifact("evaluation_report.html")
""")

    # Print sample data structure
    print("\nSample DataFrame structure:")
    print(df.head(3).to_string())

    return df, generator


if __name__ == "__main__":
    df, generator = main()
